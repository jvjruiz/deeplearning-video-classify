{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page Turner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torch_lr_finder.lr_finder:To enable mixed precision training, please install `apex`. Or you can re-install this package by the following command:\n",
      "  pip install torch-lr-finder -v --global-option=\"amp\"\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "# PyTorch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torch import optim, cuda\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "# Data science tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Image manipulations\n",
    "import PIL.Image\n",
    "# Timing utility\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import itertools\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoRecord(object):\n",
    "    \"\"\"\n",
    "    Helper class for class VideoFrameDataset. This class\n",
    "    represents a video sample's metadata.\n",
    "    Args:\n",
    "        root_datapath: the system path to the root folder\n",
    "                       of the videos.\n",
    "        row: A list with four or more elements where 1) The first\n",
    "             element is the path to the video sample's frames excluding\n",
    "             the root_datapath prefix 2) The  second element is the starting frame id of the video\n",
    "             3) The third element is the inclusive ending frame id of the video\n",
    "             4) The fourth element is the label index.\n",
    "             5) any following elements are labels in the case of multi-label classification\n",
    "    \"\"\"\n",
    "    def __init__(self, row, root_datapath):\n",
    "        self._data = row\n",
    "        self._path = os.path.join(root_datapath, row[0])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def path(self):\n",
    "        return self._path\n",
    "\n",
    "    @property\n",
    "    def num_frames(self):\n",
    "        return self.end_frame - self.start_frame + 1  # +1 because end frame is inclusive\n",
    "    @property\n",
    "    def start_frame(self):\n",
    "        return int(self._data[1])\n",
    "\n",
    "    @property\n",
    "    def end_frame(self):\n",
    "        return int(self._data[2])\n",
    "\n",
    "    @property\n",
    "    def label(self):\n",
    "        # just one label_id\n",
    "        if len(self._data) == 4:\n",
    "            return int(self._data[3])\n",
    "        # sample associated with multiple labels\n",
    "        else:\n",
    "            return [int(label_id) for label_id in self._data[3:]]\n",
    "\n",
    "class VideoFrameDataset(torch.utils.data.Dataset):\n",
    "    r\"\"\"\n",
    "    A highly efficient and adaptable dataset class for videos.\n",
    "    Instead of loading every frame of a video,\n",
    "    loads x RGB frames of a video (sparse temporal sampling) and evenly\n",
    "    chooses those frames from start to end of the video, returning\n",
    "    a list of x PIL images or ``FRAMES x CHANNELS x HEIGHT x WIDTH``\n",
    "    tensors where FRAMES=x if the ``ImglistToTensor()``\n",
    "    transform is used.\n",
    "    More specifically, the frame range [START_FRAME, END_FRAME] is divided into NUM_SEGMENTS\n",
    "    segments and FRAMES_PER_SEGMENT consecutive frames are taken from each segment.\n",
    "    Note:\n",
    "        A demonstration of using this class can be seen\n",
    "        in ``demo.py``\n",
    "        https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch\n",
    "    Note:\n",
    "        This dataset broadly corresponds to the frame sampling technique\n",
    "        introduced in ``Temporal Segment Networks`` at ECCV2016\n",
    "        https://arxiv.org/abs/1608.00859.\n",
    "    Note:\n",
    "        This class relies on receiving video data in a structure where\n",
    "        inside a ``ROOT_DATA`` folder, each video lies in its own folder,\n",
    "        where each video folder contains the frames of the video as\n",
    "        individual files with a naming convention such as\n",
    "        img_001.jpg ... img_059.jpg.\n",
    "        For enumeration and annotations, this class expects to receive\n",
    "        the path to a .txt file where each video sample has a row with four\n",
    "        (or more in the case of multi-label, see README on Github)\n",
    "        space separated values:\n",
    "        ``VIDEO_FOLDER_PATH     START_FRAME      END_FRAME      LABEL_INDEX``.\n",
    "        ``VIDEO_FOLDER_PATH`` is expected to be the path of a video folder\n",
    "        excluding the ``ROOT_DATA`` prefix. For example, ``ROOT_DATA`` might\n",
    "        be ``home\\data\\datasetxyz\\videos\\``, inside of which a ``VIDEO_FOLDER_PATH``\n",
    "        might be ``jumping\\0052\\`` or ``sample1\\`` or ``00053\\``.\n",
    "    Args:\n",
    "        root_path: The root path in which video folders lie.\n",
    "                   this is ROOT_DATA from the description above.\n",
    "        annotationfile_path: The .txt annotation file containing\n",
    "                             one row per video sample as described above.\n",
    "        num_segments: The number of segments the video should\n",
    "                      be divided into to sample frames from.\n",
    "        frames_per_segment: The number of frames that should\n",
    "                            be loaded per segment. For each segment's\n",
    "                            frame-range, a random start index or the\n",
    "                            center is chosen, from which frames_per_segment\n",
    "                            consecutive frames are loaded.\n",
    "        imagefile_template: The image filename template that video frame files\n",
    "                            have inside of their video folders as described above.\n",
    "        transform: Transform pipeline that receives a list of PIL images/frames.\n",
    "        random_shift: Whether the frames from each segment should be taken\n",
    "                      consecutively starting from the center of the segment, or\n",
    "                      consecutively starting from a random location inside the\n",
    "                      segment range.\n",
    "        test_mode: Whether this is a test dataset. If so, chooses\n",
    "                   frames from segments with random_shift=False.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root_path: str,\n",
    "                 annotationfile_path: str,\n",
    "                 num_segments: int = 3,\n",
    "                 frames_per_segment: int = 1,\n",
    "                 imagefile_template: str='img_{:05d}.jpg',\n",
    "                 transform = None,\n",
    "                 random_shift: bool = True,\n",
    "                 test_mode: bool = False):\n",
    "        super(VideoFrameDataset, self).__init__()\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.annotationfile_path = annotationfile_path\n",
    "        self.num_segments = num_segments\n",
    "        self.frames_per_segment = frames_per_segment\n",
    "        self.imagefile_template = imagefile_template\n",
    "        self.transform = transform\n",
    "        self.random_shift = random_shift\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "        self._parse_list()\n",
    "\n",
    "    def _load_image(self, directory, idx):\n",
    "        return [PIL.Image.open(os.path.join(directory, self.imagefile_template.format(idx))).convert('RGB')]\n",
    "\n",
    "    def _parse_list(self):\n",
    "        self.video_list = [VideoRecord(x.strip().split(' '), self.root_path) for x in open(self.annotationfile_path)]\n",
    "\n",
    "    def _sample_indices(self, record):\n",
    "        \"\"\"\n",
    "        For each segment, chooses an index from where frames\n",
    "        are to be loaded from.\n",
    "        Args:\n",
    "            record: VideoRecord denoting a video sample.\n",
    "        Returns:\n",
    "            List of indices of where the frames of each\n",
    "            segment are to be loaded from.\n",
    "        \"\"\"\n",
    "\n",
    "        segment_duration = (record.num_frames - self.frames_per_segment + 1) // self.num_segments\n",
    "        if segment_duration > 0:\n",
    "            offsets = np.multiply(list(range(self.num_segments)), segment_duration) + np.random.randint(segment_duration, size=self.num_segments)\n",
    "\n",
    "        # edge cases for when a video has approximately less than (num_frames*frames_per_segment) frames.\n",
    "        # random sampling in that case, which will lead to repeated frames.\n",
    "        else:\n",
    "            offsets = np.sort(np.random.randint(record.num_frames, size=self.num_segments))\n",
    "\n",
    "        return offsets\n",
    "\n",
    "    def _get_val_indices(self, record):\n",
    "        \"\"\"\n",
    "        For each segment, finds the center frame index.\n",
    "        Args:\n",
    "            record: VideoRecord denoting a video sample.\n",
    "        Returns:\n",
    "             List of indices of segment center frames.\n",
    "        \"\"\"\n",
    "        if record.num_frames > self.num_segments + self.frames_per_segment - 1:\n",
    "            offsets = self._get_test_indices(record)\n",
    "\n",
    "        # edge case for when a video does not have enough frames\n",
    "        else:\n",
    "            offsets = np.sort(np.random.randint(record.num_frames, size=self.num_segments))\n",
    "\n",
    "        return offsets\n",
    "\n",
    "    def _get_test_indices(self, record):\n",
    "        \"\"\"\n",
    "        For each segment, finds the center frame index.\n",
    "        Args:\n",
    "            record: VideoRecord denoting a video sample\n",
    "        Returns:\n",
    "            List of indices of segment center frames.\n",
    "        \"\"\"\n",
    "\n",
    "        tick = (record.num_frames - self.frames_per_segment + 1) / float(self.num_segments)\n",
    "\n",
    "        offsets = np.array([int(tick / 2.0 + tick * x) for x in range(self.num_segments)])\n",
    "\n",
    "        return offsets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        For video with id index, loads self.NUM_SEGMENTS * self.FRAMES_PER_SEGMENT\n",
    "        frames from evenly chosen locations.\n",
    "        Args:\n",
    "            index: Video sample index.\n",
    "        Returns:\n",
    "            a list of PIL images or the result\n",
    "            of applying self.transform on this list if\n",
    "            self.transform is not None.\n",
    "        \"\"\"\n",
    "        record = self.video_list[index]\n",
    "\n",
    "        if not self.test_mode:\n",
    "            segment_indices = self._sample_indices(record) if self.random_shift else self._get_val_indices(record)\n",
    "        else:\n",
    "            segment_indices = self._get_test_indices(record)\n",
    "\n",
    "        return self._get(record, segment_indices)\n",
    "\n",
    "    def _get(self, record, indices):\n",
    "        \"\"\"\n",
    "        Loads the frames of a video at the corresponding\n",
    "        indices.\n",
    "        Args:\n",
    "            record: VideoRecord denoting a video sample.\n",
    "            indices: Indices at which to load video frames from.\n",
    "        Returns:\n",
    "            1) A list of PIL images or the result\n",
    "            of applying self.transform on this list if\n",
    "            self.transform is not None.\n",
    "            2) An integer denoting the video label.\n",
    "        \"\"\"\n",
    "\n",
    "        indices = indices + record.start_frame\n",
    "        images = list()\n",
    "        image_indices = list()\n",
    "        for seg_ind in indices:\n",
    "            frame_index = int(seg_ind)\n",
    "            for i in range(self.frames_per_segment):\n",
    "                seg_img = self._load_image(record.path, frame_index)\n",
    "                images.extend(seg_img)\n",
    "                image_indices.append(frame_index)\n",
    "                if frame_index < record.end_frame:\n",
    "                    frame_index += 1\n",
    "\n",
    "        # sort images by index in case of edge cases where segments overlap each other because the overall\n",
    "        # video is too short for num_segments*frames_per_segment indices.\n",
    "        # _, images = (list(sorted_list) for sorted_list in zip(*sorted(zip(image_indices, images))))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            images = self.transform(images)\n",
    "\n",
    "\n",
    "        images = torch.transpose(images, 0, 1)\n",
    "        print(images.shape)\n",
    "        return images, record.label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_list)\n",
    "\n",
    "class ImglistToTensor(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Converts a list of PIL images in the range [0,255] to a torch.FloatTensor\n",
    "    of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1].\n",
    "    Can be used as first transform for ``VideoFrameDataset``.\n",
    "    \"\"\"\n",
    "    def forward(self, img_list):\n",
    "        \"\"\"\n",
    "        Converts each PIL image in a list to\n",
    "        a torch Tensor and stacks them into\n",
    "        a single tensor.\n",
    "        Args:\n",
    "            img_list: list of PIL images.\n",
    "        Returns:\n",
    "            tensor of size ``NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``\n",
    "        \"\"\"\n",
    "        tensor = torch.stack([transforms.functional.to_tensor(pic) for pic in img_list])\n",
    "        \n",
    "        print('first transofrm', tensor.shape)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../data/images/training'\n",
    "test_dir = '../data/images/testing'\n",
    "valid_dir = '../data/images/validation'\n",
    "train_annotations = '../data/images/training/annotations.txt'\n",
    "test_annotations = '../data/images/testing/annotations.txt'\n",
    "valid_annotations = '../data/images/testing/annotations.txt'\n",
    "batch_size = 2\n",
    "\n",
    "num_segments = 3\n",
    "frames_per_segment = 3\n",
    "\n",
    "# Datasets from each folder\n",
    "image_data = {\n",
    "    'train': VideoFrameDataset(\n",
    "                root_path=train_dir,\n",
    "                annotationfile_path=train_annotations,\n",
    "                num_segments=num_segments,\n",
    "                frames_per_segment=frames_per_segment,\n",
    "                imagefile_template='{:01d}.jpg',\n",
    "                transform=image_transforms['train'],\n",
    "                random_shift=True,\n",
    "                test_mode=False\n",
    "            ),\n",
    "    'val': VideoFrameDataset(\n",
    "                root_path=valid_dir,\n",
    "                annotationfile_path=valid_annotations,\n",
    "                num_segments=num_segments,\n",
    "                frames_per_segment=frames_per_segment,\n",
    "                imagefile_template='{:01d}.jpg',\n",
    "                transform=image_transforms['val'],\n",
    "                random_shift=True,\n",
    "                test_mode=False\n",
    "            ),\n",
    "    'test': VideoFrameDataset(\n",
    "                root_path=test_dir,\n",
    "                annotationfile_path=test_annotations,\n",
    "                num_segments=num_segments,\n",
    "                frames_per_segment=frames_per_segment,\n",
    "                imagefile_template='{:01d}.jpg',\n",
    "                transform=image_transforms['test'],\n",
    "                random_shift=True,\n",
    "                test_mode=False\n",
    "            )\n",
    "}\n",
    "\n",
    "# Dataloader iterators\n",
    "dataloaders = {\n",
    "    'train': DataLoader(image_data['train'], shuffle=True),\n",
    "    'val': DataLoader(image_data['val'], shuffle=True),\n",
    "    'test': DataLoader(image_data['test'], shuffle=False)\n",
    "}\n",
    "\n",
    "image_transforms = {\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        ImglistToTensor(),\n",
    "        transforms.Resize(128),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    # Validation does not use augmentation\n",
    "    'val':\n",
    "    transforms.Compose([\n",
    "        ImglistToTensor(),        \n",
    "        transforms.Resize(128),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    # Test does not use augmentation\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        ImglistToTensor(),        \n",
    "        transforms.Resize(128),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been trained for: 0 epochs.\n",
      "\n",
      "first transofrm torch.Size([9, 3, 1920, 1080])\n",
      "torch.Size([3, 9, 227, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 9, 3, 3, 3], expected input[1, 3, 9, 227, 128] to have 9 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-3afdae8249d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mearly_stopping_patience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0moverfit_patience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     )\n",
      "\u001b[1;32m<ipython-input-18-d4d1905820bc>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, train_loader, valid_loader, save_file_name, checkpoint_file_name, early_stopping_patience, overfit_patience, n_epochs, valid_every)\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;31m#predicted outpouts are log probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;31m# loss and backpropagation of gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-328e9b05f8f4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    571\u001b[0m                             self.dilation, self.groups)\n\u001b[0;32m    572\u001b[0m         return F.conv3d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 573\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 9, 3, 3, 3], expected input[1, 3, 9, 227, 128] to have 9 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dataloaders['train'],\n",
    "    dataloaders['test'],\n",
    "    save_file_name=save_file_name,\n",
    "    checkpoint_file_name=checkpoint_file_name,\n",
    "    early_stopping_patience=50,\n",
    "    overfit_patience=10,\n",
    "    n_epochs=200\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Data Wrangling and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of data:\n",
    "This dataset contains video frame images for pages being turned and not being turned. There are:\n",
    "1. 65 training videos for flipping\n",
    "2. 58 training videos for not flipping\n",
    "3. 65 testing videos for flipping\n",
    "4. 58 testing videos for flipping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Samples Per Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_image_data(root_dir, image_dir):\n",
    "    # root_dir = root directory of the image data\n",
    "    # food_dir is the specific food directory you wish to gather the data from\n",
    "    files_in_folder = os.listdir(os.path.join(root_dir,image_dir))\n",
    "    random_image = np.random.choice(files_in_folder)\n",
    "    return plt.imread(os.path.join(root_dir,image_dir,random_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # setup root directory and grid size\n",
    "# root_dir = '../data/images/training'\n",
    "# row = 5\n",
    "# col = 5\n",
    "\n",
    "# # initiate subplot and configure title\n",
    "# fig, ax = plt.subplots(row,col,figsize=(20,10))\n",
    "# fig.suptitle(\"Random Images for Flipping\", y=.95, fontsize=24)\n",
    "# plt.setp(ax, xticks=[],yticks=[])\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# # iterate through each category of food and assign a random image to a spot on the grid\n",
    "# for i in range(row):\n",
    "#     for j in range(col):\n",
    "#         try:\n",
    "#             image_dir = 'flip'\n",
    "#         except:\n",
    "#             break\n",
    "#         img = gather_image_data(root_dir,image_dir)\n",
    "#         ax[i][j].imshow(img)\n",
    "\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setup root directory and grid size\n",
    "# root_dir = '../data/images/training'\n",
    "# row = 5\n",
    "# col = 5\n",
    "\n",
    "# # initiate subplot and configure title\n",
    "# fig, ax = plt.subplots(row,col,figsize=(20,10))\n",
    "# fig.suptitle(\"Random Images for Not Flipping\", y=.95, fontsize=24)\n",
    "# plt.setp(ax, xticks=[],yticks=[])\n",
    "# plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# # iterate through each category of food and assign a random image to a spot on the grid\n",
    "# for i in range(row):\n",
    "#     for j in range(col):\n",
    "#         try:\n",
    "#             image_dir = 'notflip'\n",
    "#         except:\n",
    "#             break\n",
    "#         img = gather_image_data(root_dir,image_dir)\n",
    "#         ax[i][j].imshow(img)\n",
    "\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Train and Test images into respective folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create functions for copying files and ignoring files\n",
    "\n",
    "def copytree(src, dst, ignored_ids = None):\n",
    "    # src = source directory\n",
    "    # dst = destination directory of copy\n",
    "    # ignore = ignore function that provides list of id's to ignore based on testing or training set\n",
    "    \n",
    "    # if destination directory does not exist, create directory\n",
    "    if not os.path.exists(dst):\n",
    "        os.makedirs(dst)\n",
    "        shutil.copystat(src, dst)\n",
    "    \n",
    "    # get list of directories in current directory\n",
    "    directory_items = os.listdir(src)\n",
    "    # filter out items to be ignored\n",
    "    directory_items = [x for x in directory_items if x not in ignored_ids]\n",
    "    # for each item in directory, copy into destination\n",
    "    for item in directory_items:\n",
    "        source = os.path.join(src, item)\n",
    "        destination = os.path.join(dst, item)\n",
    "        # if item is a directory, recurisvely call this function \n",
    "        if os.path.isdir(source):\n",
    "            print(source)\n",
    "            copytree(source, destination, ignored_ids)\n",
    "        # copy item to destination\n",
    "        else:\n",
    "            shutil.copy2(source, destination)\n",
    "            \n",
    "def sort_videos(src, action=\"flip\"):\n",
    "    src = os.path.join(src, action)\n",
    "        \n",
    "    # get list of files in folder\n",
    "    directory_items = os.listdir(src)\n",
    "    # iterate through files\n",
    "    last_video_id = 99999\n",
    "    count = 0\n",
    "    for video in directory_items:\n",
    "#         print(directory_items)\n",
    "        video_id = video.split('_')[0]\n",
    "#         frame = video.split('_')[1]\n",
    "        if video_id != last_video_id:\n",
    "            count = 0\n",
    "            last_video_id = video_id\n",
    "        destination = os.path.join(src, video_id)\n",
    "        # on new video\n",
    "        if not os.path.exists(destination):\n",
    "            # create new folder\n",
    "            os.makedirs(destination)\n",
    "            shutil.copystat(src, destination)\n",
    "        # move images to folder\n",
    "        shutil.copy2(os.path.join(src,video), os.path.join(destination, \"{}.jpg\".format(str(count))))\n",
    "        count += 1\n",
    "\n",
    "\n",
    "def sort_images():\n",
    "    train_dir = '../data/images/training'\n",
    "    test_dir = '../data/images/testing'\n",
    "    valid_dir = '../data/valid'\n",
    "    \n",
    "    if not os.path.isdir('../data/train'):\n",
    "        sort_videos(train_dir, 'flip')\n",
    "        sort_videos(train_dir, 'notflip')\n",
    "    else:\n",
    "        print('Train files already copied into separate folders.')\n",
    "\n",
    "#     if not os.path.isdir('../data/test'):\n",
    "#         sort_videos(test_dir, 'flip')\n",
    "#         sort_videos(test_dir, 'notflip')\n",
    "#     else:\n",
    "#         print('Test files already copied into separate folders.')\n",
    "        \n",
    "def annotate_videos(src):\n",
    "#     jumping/0001 1 17 0\n",
    "    annotations = []\n",
    "    \n",
    "    for action in os.listdir(src):\n",
    "        action_id = 0 if action == 'flip' else 1\n",
    "        for video in os.listdir(os.path.join(src, action)):\n",
    "            print(video)\n",
    "            images = os.listdir(os.path.join(src,action, video))\n",
    "            print(images)\n",
    "            first = images[0].split('.')[0].split('.')[0]\n",
    "            last = 0\n",
    "            for item in images:\n",
    "                num = item.split('.')[0]\n",
    "                if int(num) > last:\n",
    "                    last = int(num)\n",
    "            annotation = \"{}/{} {} {} {}\".format(action, video, first, last, action_id)\n",
    "            annotations.append(annotation)\n",
    "    with open(os.path.join(src, 'annotations.txt'), 'w') as f:\n",
    "        for annotation in annotations:\n",
    "            f.write(\"%s\\n\" % annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sort_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0003\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0004\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0005\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0006\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0007\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0008\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0009\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0010\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0011\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0012\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0013\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0014\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0015\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0016\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0017\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0018\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0019\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0020\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0021\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0022\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0028\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0029\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0030\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '30.jpg', '31.jpg', '32.jpg', '33.jpg', '34.jpg', '35.jpg', '36.jpg', '37.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0036\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0037\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0038\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '30.jpg', '31.jpg', '32.jpg', '33.jpg', '34.jpg', '35.jpg', '36.jpg', '37.jpg', '38.jpg', '39.jpg', '4.jpg', '40.jpg', '41.jpg', '42.jpg', '43.jpg', '44.jpg', '45.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0039\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0040\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0041\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0042\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0044\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0045\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0001\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0002\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0003\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0004\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '30.jpg', '31.jpg', '32.jpg', '33.jpg', '34.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0005\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0006\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0007\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0008\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0009\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0010\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0011\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0015\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0016\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0019\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0020\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0023\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0024\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0025\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0026\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0027\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0028\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0029\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0030\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0031\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0032\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0033\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0034\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0035\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '30.jpg', '31.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0036\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0046\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0047\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0048\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0052\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0053\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0054\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0055\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0037\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0038\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0039\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0040\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0041\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '30.jpg', '31.jpg', '32.jpg', '33.jpg', '34.jpg', '35.jpg', '36.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0042\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0043\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0044\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0045\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0046\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0056\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0057\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0058\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0059\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0060\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '30.jpg', '31.jpg', '32.jpg', '33.jpg', '34.jpg', '35.jpg', '36.jpg', '37.jpg', '38.jpg', '39.jpg', '4.jpg', '40.jpg', '41.jpg', '42.jpg', '43.jpg', '44.jpg', '45.jpg', '46.jpg', '47.jpg', '48.jpg', '49.jpg', '5.jpg', '50.jpg', '51.jpg', '52.jpg', '53.jpg', '54.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0061\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0062\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0065\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0047\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0048\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0049\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0050\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0051\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0052\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0053\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0054\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0055\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0056\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0057\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n",
      "0058\n",
      "['0.jpg', '1.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '2.jpg', '20.jpg', '21.jpg', '22.jpg', '23.jpg', '24.jpg', '25.jpg', '26.jpg', '27.jpg', '28.jpg', '29.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg']\n"
     ]
    }
   ],
   "source": [
    "train_dir = '../data/images/training'\n",
    "test_dir = '../data/images/testing'\n",
    "valid_dir = '../data/images/validation'\n",
    "\n",
    "annotate_videos(train_dir)\n",
    "annotate_videos(test_dir)\n",
    "annotate_videos(valid_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "### Initalize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on gpu: True\n",
      "1 gpus detected.\n"
     ]
    }
   ],
   "source": [
    "# Location of data\n",
    "data_dir = '../data/images/'\n",
    "train_dir = data_dir + 'train/'\n",
    "# valid_dir = data_dir + 'valid/'\n",
    "test_dir = data_dir + 'test/'\n",
    "\n",
    "save_file_name = 'page-turner.pt'\n",
    "checkpoint_file_name = 'page-turner-check.pt'\n",
    "\n",
    "# Change to fit hardware\n",
    "batch_size = 1\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "# Whether to train on a gpu\n",
    "train_on_gpu = cuda.is_available()\n",
    "print(f'Train on gpu: {train_on_gpu}')\n",
    "\n",
    "# Number of gpus\n",
    "if train_on_gpu:\n",
    "    gpu_count = cuda.device_count()\n",
    "    print(f'{gpu_count} gpus detected.')\n",
    "    if gpu_count > 1:\n",
    "        multi_gpu = True\n",
    "    else:\n",
    "        multi_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = {\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        ImglistToTensor(),\n",
    "        transforms.Resize(128),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    # Validation does not use augmentation\n",
    "    'val':\n",
    "    transforms.Compose([\n",
    "        ImglistToTensor(),        \n",
    "        transforms.Resize(128),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    # Test does not use augmentation\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        ImglistToTensor(),        \n",
    "        transforms.Resize(128),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../data/images/training'\n",
    "test_dir = '../data/images/testing'\n",
    "valid_dir = '../data/images/validation'\n",
    "train_annotations = '../data/images/training/annotations.txt'\n",
    "test_annotations = '../data/images/testing/annotations.txt'\n",
    "valid_annotations = '../data/images/testing/annotations.txt'\n",
    "batch_size = 2\n",
    "\n",
    "num_segments = 3\n",
    "frames_per_segment = 3\n",
    "\n",
    "# Datasets from each folder\n",
    "image_data = {\n",
    "    'train': VideoFrameDataset(\n",
    "                root_path=train_dir,\n",
    "                annotationfile_path=train_annotations,\n",
    "                num_segments=num_segments,\n",
    "                frames_per_segment=frames_per_segment,\n",
    "                imagefile_template='{:01d}.jpg',\n",
    "                transform=image_transforms['train'],\n",
    "                random_shift=True,\n",
    "                test_mode=False\n",
    "            ),\n",
    "    'val': VideoFrameDataset(\n",
    "                root_path=valid_dir,\n",
    "                annotationfile_path=valid_annotations,\n",
    "                num_segments=num_segments,\n",
    "                frames_per_segment=frames_per_segment,\n",
    "                imagefile_template='{:01d}.jpg',\n",
    "                transform=image_transforms['val'],\n",
    "                random_shift=True,\n",
    "                test_mode=False\n",
    "            ),\n",
    "    'test': VideoFrameDataset(\n",
    "                root_path=test_dir,\n",
    "                annotationfile_path=test_annotations,\n",
    "                num_segments=num_segments,\n",
    "                frames_per_segment=frames_per_segment,\n",
    "                imagefile_template='{:01d}.jpg',\n",
    "                transform=image_transforms['test'],\n",
    "                random_shift=True,\n",
    "                test_mode=False\n",
    "            )\n",
    "}\n",
    "\n",
    "# Dataloader iterators\n",
    "dataloaders = {\n",
    "    'train': DataLoader(image_data['train'], shuffle=True),\n",
    "    'val': DataLoader(image_data['val'], shuffle=True),\n",
    "    'test': DataLoader(image_data['test'], shuffle=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReferences\\n----------\\n[1] Tran, Du, et al. \"Learning spatiotemporal features with 3d convolutional networks.\" \\nProceedings of the IEEE international conference on computer vision. 2015.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class C3D(nn.Module):\n",
    "    \"\"\"\n",
    "    The C3D network as described in [1].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(C3D, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv3d(9, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "\n",
    "        self.conv3a = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv3b = nn.Conv3d(256, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "\n",
    "        self.conv4a = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv4b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "\n",
    "        self.conv5a = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.conv5b = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1))\n",
    "\n",
    "        self.fc6 = nn.Linear(8192, 4096)\n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "        self.fc8 = nn.Linear(4096, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        h = self.relu(self.conv1(x))\n",
    "        h = self.pool1(h)\n",
    "\n",
    "        h = self.relu(self.conv2(h))\n",
    "        h = self.pool2(h)\n",
    "\n",
    "        h = self.relu(self.conv3a(h))\n",
    "        h = self.relu(self.conv3b(h))\n",
    "        h = self.pool3(h)\n",
    "\n",
    "        h = self.relu(self.conv4a(h))\n",
    "        h = self.relu(self.conv4b(h))\n",
    "        h = self.pool4(h)\n",
    "\n",
    "        h = self.relu(self.conv5a(h))\n",
    "        h = self.relu(self.conv5b(h))\n",
    "        h = self.pool5(h)\n",
    "\n",
    "        h = h.view(-1, 8192)\n",
    "        h = self.relu(self.fc6(h))\n",
    "        h = self.dropout(h)\n",
    "        h = self.relu(self.fc7(h))\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        logits = self.fc8(h)\n",
    "        probs = self.softmax(logits)\n",
    "\n",
    "        return probs\n",
    "\n",
    "\"\"\"\n",
    "References\n",
    "----------\n",
    "[1] Tran, Du, et al. \"Learning spatiotemporal features with 3d convolutional networks.\" \n",
    "Proceedings of the IEEE international conference on computer vision. 2015.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C3D(\n",
       "  (conv1): Conv3d(9, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool1): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool2): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3a): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv3b): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4a): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv4b): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5a): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv5b): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool5): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
       "  (fc6): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "  (fc7): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (fc8): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (softmax): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = C3D()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace Last layer with fully connected layer configured for the task at hand\n",
    "\n",
    "To finish modifying the ResNet50 pre-trained network I replace the last fully connected layer and replace it with two fully connected layers and a LeakyReLU function after the first one. I chose to use two so I could down sample the outgoing features of the pre-trained model and the new layers to first the pixel size of the image, then to the number of classes in the dataset.\n",
    "\n",
    "If I had more time I would experiment with the configuration of these last layers as I have seen increases and decreases in the accuracy simply based on the layers a the end of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_classes = 2\n",
    "\n",
    "# n_inputs = model.fc8.in_features\n",
    "\n",
    "# classifier = nn.Sequential(\n",
    "#     nn.Linear(n_inputs,image_size),\n",
    "#     nn.LeakyReLU(),\n",
    "#     nn.Linear(image_size,n_classes)\n",
    "#     )\n",
    "\n",
    "# model.fc8 = classifier\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move model to GPU\n",
    "\n",
    "This will allow the model to train quicker by taking advantage of a GPU if the learning environment has one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if train_on_gpu:\n",
    "    model = model.to('cuda')\n",
    "\n",
    "if multi_gpu:\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loss and Optimizer\n",
    "\n",
    "The loss function I use is Cross Entropy Loss because it is commonly used in classification models. It works by comparing predicted probabilities of the datapoint is and the actual label. The loss increases as the difference predicted probability is from the actual point.\n",
    "\n",
    "For the optimizer I chose to use Stochastic Gradient Descent (SGD). At first I used the ADAM optimizer to prototype the model, however once I knew everything else was in place and working I switched over to SGD to give me a more fine tuned approach to exploring the gradient of the data. \n",
    "\n",
    "Along with using SGD as my optimizer, I pair it with a Cyclical Learning Rate scheduler. This allows the model to get a better lay of the land by cycling between a lower and upper bound learning rate. Doing this allows the model to explore the gradient curve in more than one direction and hopefully avoid getting stuck in a local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be using negative log likelihood as the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# we will be using the Adam optimizer as our optimizer\n",
    "optimizer = optim.Adam(model.fc8.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "The model will train in two phases.\n",
    "In phase one we train the model while only updating the weights on the new classifier layers that we added to the pre-trained ResNet50 model. This will allow training to focus on the last few layers and ensure that they are trained accordingly with the data. This phase will continue until the training loss becomes greater than the validation loss for a certain amount of training epochs, in this case 10. I use that as a metric to stop training because it is an indicator that the model is becoming overfitted to the training data. The validation loss is naturally smaller than the training loss in this example because of the amount of image transformations we are doing before feeding the images into the model. I chose to do this to allow the model to be able to generalize better to unseen data.\n",
    "\n",
    "Phase two of training will be almost identical to phase one, with the exception of unfreezing all layers of the model to allow the training process to update the weights of the rest of the layers. This will continue until the training loss becomes smaller than the valid loss for a certain amount of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                save_file_name,\n",
    "                checkpoint_file_name,\n",
    "                early_stopping_patience=100,\n",
    "                overfit_patience=15,\n",
    "                n_epochs=25,\n",
    "                valid_every=2\n",
    "               ):\n",
    "    \"\"\"Train a PyTorch Model\n",
    "\n",
    "    Params\n",
    "    --------\n",
    "        model (PyTorch model): cnn to train\n",
    "        criterion (PyTorch loss): objective to minimize\n",
    "        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
    "        train_loader (PyTorch dataloader): training dataloader to iterate through\n",
    "        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n",
    "        save_file_name (str ending in '.pt'): file path to save the model state dict\n",
    "        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
    "        n_epochs (int): maximum number of training epochs\n",
    "        valid_every (int): frequency of epochs to validate model\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        model (PyTorch model): trained cnn with best weights\n",
    "        history (DataFrame): history of train and validation loss and accuracy\n",
    "    \"\"\"\n",
    "    # early stopping initializaiton\n",
    "    epochs_no_improve = 0\n",
    "    epochs_overfit = 0\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    valid_max_acc = 0\n",
    "    history = []\n",
    "    \n",
    "    # number of epochs already trained (if using loaded in model weights)\n",
    "    try:\n",
    "        print(\"Model has been trained for: {} epochs.\\n\".format(model.epochs))\n",
    "    except:\n",
    "        model.epochs = 0\n",
    "        print(\"Starting training from scratch.\\n\")\n",
    "        \n",
    "    overall_start = timer()\n",
    "    \n",
    "    #Main loop\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        #keep track of training and validation loss of each epoch\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        train_acc = 0\n",
    "        valid_acc = 0\n",
    "        \n",
    "        #set to training\n",
    "        model.train()\n",
    "        start = timer()\n",
    "        \n",
    "        # training loop\n",
    "        for ii, (data, target) in enumerate(train_loader):\n",
    "            #tensors to gpu\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            #predicted outpouts are log probabilities\n",
    "            output = model(data)\n",
    "            \n",
    "            # loss and backpropagation of gradients\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # track train loss by multiplying average loss by number of examples in batch\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # calculate accuracy by finding max log probability\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            # need to convert correct tensor from int to float to average\n",
    "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "            # multiply average accuracy times the number of examples in batch\n",
    "            train_acc += accuracy.item() * data.size(0)\n",
    "            \n",
    "            # Track training progress\n",
    "            print(\n",
    "                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n",
    "                end='\\r')\n",
    "        # after training loop ends\n",
    "        else:\n",
    "            model.epochs += 1\n",
    "            \n",
    "            if model.epochs > 1 and (model.epochs % valid_every == 0): \n",
    "                # don't need to keep track of gradients\n",
    "                with torch.no_grad():\n",
    "                    # set to evaluation mode\n",
    "                    model.eval()\n",
    "\n",
    "                    #validation loop\n",
    "                    for data, target in valid_loader:\n",
    "                        #tensors to gpu\n",
    "                        if train_on_gpu:\n",
    "                            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                        # Forward pass\n",
    "                        output = model(data)\n",
    "\n",
    "                        # validation loss \n",
    "                        loss = criterion(output, target)\n",
    "                        # multiply average loss times the number of examples in batch\n",
    "                        valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                        # calculate validation accuracy\n",
    "                        _, pred = torch.max(output, dim=1)\n",
    "                        correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "                        accuracy = torch.mean(\n",
    "                            correct_tensor.type(torch.FloatTensor))\n",
    "                        # multiply average accuracy times the number of examples\n",
    "                        valid_acc += accuracy.item() * data.size(0)\n",
    "\n",
    "                    # calculate average losses\n",
    "                    train_loss = train_loss / (len(train_loader.dataset))\n",
    "                    valid_loss = valid_loss / (len(valid_loader.dataset))\n",
    "\n",
    "                    # calculate average accuracy\n",
    "                    train_acc = train_acc / (len(train_loader.dataset))\n",
    "                    valid_acc = valid_acc / (len(valid_loader.dataset))\n",
    "\n",
    "\n",
    "                    history.append([train_loss, valid_loss, train_acc, valid_acc, model.epochs])\n",
    "\n",
    "                    print(\n",
    "                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
    "                    )\n",
    "                    print(\n",
    "                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n",
    "                    )\n",
    "\n",
    "                    # save the model if validation loss decreases\n",
    "                    if valid_loss < valid_loss_min:\n",
    "                        print(\"Valid loss decreased ({:.6f} --> {:.6f}). Saving model...\".format(valid_loss_min, valid_loss))\n",
    "\n",
    "                        # save model\n",
    "                        torch.save(model.state_dict(), save_file_name)\n",
    "\n",
    "                        checkpoint = {\n",
    "                            \"model\": model,\n",
    "                            \"criterion\": criterion,\n",
    "                            \"epochs\": model.epochs,\n",
    "                            \"optimizer_state\": optimizer.state_dict(),\n",
    "                            \"model_state\": model.state_dict(),\n",
    "                            \"valid_loss_min\": valid_loss\n",
    "                        }\n",
    "                        torch.save(checkpoint, checkpoint_file_name)\n",
    "\n",
    "                        # track improvements\n",
    "                        epochs_no_improve = 0\n",
    "                        epochs_overfit = 0\n",
    "                        valid_loss_min = valid_loss\n",
    "                        valid_best_acc = valid_acc\n",
    "                        best_epoch = epoch\n",
    "\n",
    "                    # otherwise increment count of epochs with no improvement\n",
    "                    elif train_loss < valid_loss:\n",
    "                        epochs_overfit += 1\n",
    "                        if epochs_overfit >= overfit_patience:\n",
    "                            print(f'\\n Valid loss has increased larger than training loss for {epochs_overfit} epochs')\n",
    "                            print(\n",
    "                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
    "                            )\n",
    "                            # load the best state dict\n",
    "                            model.load_state_dict(torch.load(save_file_name))\n",
    "                            # attach the optimizer\n",
    "                            model.optimizer = optimizer\n",
    "\n",
    "                            # format history\n",
    "                            history = pd.DataFrame(\n",
    "                                    history,\n",
    "                                    columns=[\n",
    "                                        'train_loss', 'valid_loss', 'train_acc',\n",
    "                                        'valid_acc', 'epochs'\n",
    "                                    ])\n",
    "                            return model, history\n",
    "\n",
    "                    else:\n",
    "                        epochs_no_improve += 1\n",
    "                        #trigger early stopping\n",
    "                        # this should be not going bad\n",
    "                        if (epochs_no_improve >= early_stopping_patience):\n",
    "                            print(\n",
    "                                f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
    "                            )\n",
    "                            total_time = timer() - overall_start\n",
    "                            print(\n",
    "                                f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
    "                            )\n",
    "\n",
    "                            # load the best state dict\n",
    "                            model.load_state_dict(torch.load(save_file_name))\n",
    "                            # attach the optimizer\n",
    "                            model.optimizer = optimizer\n",
    "\n",
    "                            # format history\n",
    "                            history = pd.DataFrame(\n",
    "                                    history,\n",
    "                                    columns=[\n",
    "                                        'train_loss', 'valid_loss', 'train_acc',\n",
    "                                        'valid_acc', 'epochs'\n",
    "                                    ])\n",
    "                            return model, history\n",
    "                        \n",
    "    model.optimizer = optimizer\n",
    "    total_time = timer() - overall_start\n",
    "    print(\n",
    "        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
    "    )\n",
    "    print(\n",
    "        f'{total_time:.2f} total seconds elapsed. {total_time / (model.epochs):.2f} seconds per epoch.'\n",
    "    )\n",
    "    # Format history\n",
    "    history = pd.DataFrame(\n",
    "        history,\n",
    "        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc','epochs'])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been trained for: 0 epochs.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 16, 3, 3, 3], expected input[1, 9, 3, 227, 128] to have 16 channels, but got 9 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-3afdae8249d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mearly_stopping_patience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0moverfit_patience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     )\n",
      "\u001b[1;32m<ipython-input-18-d4d1905820bc>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, train_loader, valid_loader, save_file_name, checkpoint_file_name, early_stopping_patience, overfit_patience, n_epochs, valid_every)\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;31m#predicted outpouts are log probabilities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;31m# loss and backpropagation of gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-58e02d9a10b6>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    571\u001b[0m                             self.dilation, self.groups)\n\u001b[0;32m    572\u001b[0m         return F.conv3d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 573\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 16, 3, 3, 3], expected input[1, 9, 3, 227, 128] to have 16 channels, but got 9 channels instead"
     ]
    }
   ],
   "source": [
    "model, history = train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dataloaders['train'],\n",
    "    dataloaders['test'],\n",
    "    save_file_name=save_file_name,\n",
    "    checkpoint_file_name=checkpoint_file_name,\n",
    "    early_stopping_patience=50,\n",
    "    overfit_patience=10,\n",
    "    n_epochs=200\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history['train_acc'])\n",
    "plt.plot(history['valid_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.xlim(1,200)\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('train_valid_accuracy.png')\n",
    "plt.show();\n",
    "\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history['train_loss'])\n",
    "plt.plot(history['valid_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.xlim(1,200)\n",
    "plt.legend(['train', 'valid'], loc='upper left')\n",
    "plt.savefig('train_valid_loss.png')\n",
    "\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze all layers of the model to allow them to start training\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, history = train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    dataloaders['train'],\n",
    "    dataloaders['val'],\n",
    "    save_file_name=save_file_name,\n",
    "    checkpoint_file_name=checkpoint_file_name,\n",
    "    early_stopping_patience=50,\n",
    "    overfit_patience=10,\n",
    "    n_epochs=200\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    dataloaders['train'],\n",
    "    dataloaders['val'],\n",
    "    save_file_name=save_file_name,\n",
    "    checkpoint_file_name=checkpoint_file_name,\n",
    "    early_stopping_patience=50,\n",
    "    overfit_patience=10,\n",
    "    n_epochs=200\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels():\n",
    "    food_labels = pd.read_csv(\"../data/meta/labels.txt\", header=None)\n",
    "    food_labels = food_labels[0].tolist()\n",
    "    return food_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_on_gpu:\n",
    "    model.load_state_dict(torch.load('../saved_models/resnet50-transfer10.pt', map_location= torch.device('cuda')))\n",
    "    model.to('cuda')\n",
    "else:\n",
    "    model.load_state_dict(torch.load('../saved_models/resnet50-transfer10.pt', map_location= torch.device('cpu')))\n",
    "    model.to('cpu')\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = get_labels()\n",
    "class_correct = list(0. for i in range(len(classes)))\n",
    "class_total = list(0. for i in range(len(classes)))\n",
    "total_correct = 0\n",
    "total = 12625\n",
    "y_test = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target, path in dataloaders['test']:\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_test.extend(target.cpu().numpy().tolist())\n",
    "        y_pred.extend(predicted.cpu().numpy().tolist())\n",
    "        c = (predicted == target).squeeze()\n",
    "        total_correct += (predicted == target).sum().item()\n",
    "        for i in range(len(target)):\n",
    "            label = target[i].item()\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "print('Accuracy of the network on the 12625 test images: %d %%' % (\n",
    "    100 * total_correct / total))\n",
    "for i in range(len(classes)):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report below allows us to get a better idea of how the predictions were made per class. This will be useful for fine-tuning the model down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Classification Report')\n",
    "print(classification_report(y_test, y_pred[:12625], target_names=get_labels()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          cax=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    \n",
    "    im = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the confusion matrix\n",
    "\n",
    "The confusion matrix allows us to examine which classes get predicted as other classes and vice-versa. The diagonal line from the top left to the bottom right contains the numbers for the amount of correct identifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(35,35)\n",
    "# create an axes on the right side of ax. The width of cax will be 5%\n",
    "# of ax and the padding between cax and ax will be fixed at 0.05 inch.\n",
    "\n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_pred), classes=get_labels(),\n",
    "                      title='Confusion matrix, without normalization',\n",
    "                      cmap=plt.cm.GnBu,cax=ax)\n",
    "\n",
    "axins = inset_axes(ax,\n",
    "                   width=\"5%\",  # width = 5% of parent_bbox width\n",
    "                   height=\"100%\",  # height : 50%\n",
    "                   loc='lower left',\n",
    "                   bbox_to_anchor=(1.05, 0., 1, 1),\n",
    "                   bbox_transform=ax.transAxes,\n",
    "                   borderpad=0,\n",
    "                   )\n",
    "\n",
    "plt.colorbar(cax=axins)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confusion martrix plots the true identifications on the diagonal and the wrong classificationson every other spot. The Columns represent the predicted class and the rows represent the actual class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    return roc_auc_score(y_test, y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = multiclass_roc_auc_score(y_test, y_pred)\n",
    "print(\"Multi-Class AUC Score is: {}\".format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a multi-class AUC score of .87 which means the predictions are quite accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images_prediction(food_class,y_test, page=0):\n",
    "    page_size = 20\n",
    "    nrows = 4\n",
    "    ncols = 5\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, 12))\n",
    "    fig.set_size_inches(12, 8)\n",
    "    fig.tight_layout()\n",
    "    labels = get_labels()\n",
    "    start_i = (labels.index(food_class) * 125) + (page * 20)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        im = ax.imshow(plt.imread(image_data['test'][i+start_i][2]))\n",
    "        ax.set_axis_off()\n",
    "        ax.title.set_visible(False)\n",
    "        ax.xaxis.set_ticks([])\n",
    "        ax.yaxis.set_ticks([])\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        predicted = labels[y_pred[i+start_i]]\n",
    "        match = predicted ==  labels[y_test[start_i + i]]\n",
    "        ec = (1, .5, .5)\n",
    "        fc = (1, .8, .8)\n",
    "        if match:\n",
    "            ec = (0, .6, .1)\n",
    "            fc = (0, .7, .2)\n",
    "        # predicted label\n",
    "        ax.text(0, 350, 'P: ' + predicted, size=10, rotation=0,\n",
    "            ha=\"left\", va=\"top\",\n",
    "             bbox=dict(boxstyle=\"round\",\n",
    "                   ec=ec,\n",
    "                   fc=fc,\n",
    "                   )\n",
    "             )\n",
    "        if not match:\n",
    "            # true label\n",
    "            ax.text(0, 440, 'A: ' + labels[y_test[start_i + i]], size=10, rotation=0,\n",
    "                ha=\"left\", va=\"top\",\n",
    "                 bbox=dict(boxstyle=\"round\",\n",
    "                       ec=ec,\n",
    "                       fc=fc,\n",
    "                       )\n",
    "                 )\n",
    "    plt.subplots_adjust(left=0, wspace=1, hspace=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# change the first parameter to the class label you would like to examine.\n",
    "# if you would like to check different images of the class increase the page number\n",
    "show_images_prediction('Grilled cheese sandwich', y_test, page=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing some different pages above, it is clear that building a food classifier is not easy. With the understanding of how a CNN gets features and uses them to build predictions, it is possible to see why the model made some of the wrong predictions that it did. \n",
    "\n",
    "Some other food labels may have very similar shapes and colors, which could trick the model into thinking the image was of a different class.\n",
    "\n",
    "Or in the case of edamame, the food has very clear and defined features that allows for the very high accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the first parameter to the class label you would like to examine.\n",
    "# if you would like to check different images of the class increase the page number\n",
    "show_images_prediction('Edamame', y_test, page=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
